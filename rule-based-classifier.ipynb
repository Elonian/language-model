{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rule based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Libraries\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractIO(datafile: str):\n",
    "    I = []\n",
    "    O = []\n",
    "    with open('/Users/varunmoparthi/Desktop/nlp/data/sst-sentiment-text-threeclass/train.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            y, x = line.strip().split('|||')\n",
    "            I.append(x)\n",
    "            O.append(int(y))\n",
    "    return I, O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = extractIO('/Users/varunmoparthi/Desktop/nlp/data/sst-sentiment-text-threeclass/train.txt')\n",
    "x_test, y_test = extractIO('/Users/varunmoparthi/Desktop/nlp/data/sst-sentiment-text-threeclass/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\" The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\",\n",
       "  \" The gorgeously elaborate continuation of `` The Lord of the Rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\\\/director Peter Jackson 's expanded vision of J.R.R. Tolkien 's Middle-earth .\",\n",
       "  ' Singer\\\\/composer Bryan Adams contributes a slew of songs -- a few potential hits , a few more simply intrusive to the story -- but the whole package certainly captures the intended , er , spirit of the piece .',\n",
       "  \" You 'd think by now America would have had enough of plucky British eccentrics with hearts of gold .\",\n",
       "  ' Yet the act is still charming here .',\n",
       "  \" Whether or not you 're enlightened by any of Derrida 's lectures on `` the other '' and `` the self , '' Derrida is an undeniably fascinating and playful fellow .\",\n",
       "  ' Just the labour involved in creating the layered richness of the imagery in this chiaroscuro of madness and light is astonishing .',\n",
       "  ' Part of the charm of Satin Rouge is that it avoids the obvious with humour and lightness .',\n",
       "  \" a screenplay more ingeniously constructed than `` Memento ''\",\n",
       "  \" `` Extreme Ops '' exceeds expectations .\"],\n",
       " [1, 1, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:10], y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8544, 8544)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step-1 Feature Extraction\n",
    "\n",
    "def feature_extraction(x, good, bad):\n",
    "    words = x.split(' ')\n",
    "    feature = defaultdict(int)\n",
    "    for i in words:\n",
    "        if i in good:\n",
    "            feature['good'] += 1\n",
    "        if i in bad:\n",
    "            feature['bad'] += 1\n",
    "    feature['bias'] = 1\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature weights\n",
    "feat_weights = {'good': 1.0, 'bad': -1.5, 'bias': 0.5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step-2 Score calculation\n",
    "\n",
    "def score_calculation(feature, feat_weights):\n",
    "    score = 0\n",
    "    for name, count in feature.items():\n",
    "        score += feature[name] * feat_weights[name]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step-3 Decision function\n",
    "\n",
    "def decision_function(feature, feat_weights):\n",
    "    score = score_calculation(feature, feat_weights)\n",
    "    if score > 0:\n",
    "        return 1\n",
    "    if score < 0:\n",
    "        return -1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predictions\n",
    "\n",
    "def predictions(x, good, bad, feat_weights):\n",
    "    y_pred = []\n",
    "    for i in x:\n",
    "        feature = feature_extraction(i, good, bad)\n",
    "        y_pred.append(decision_function(feature, feat_weights))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy\n",
    "\n",
    "def accuracy(y_pred, y):\n",
    "    correct_pred = 0\n",
    "    for i,j in zip(y_pred, y):\n",
    "        if(i == j):\n",
    "            correct_pred += 1 \n",
    "    return correct_pred/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = ['good']\n",
    "bad = ['bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = predictions(x_train, good, bad, feat_weights)\n",
    "y_test_pred = predictions(x_test, good, bad, feat_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8544, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_pred), y_train_pred[:10], y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 3610, 0: 1624, -1: 3310})"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = defaultdict(int)\n",
    "for y in y_train:\n",
    "    label[y] += 1\n",
    "\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 8406, -1: 123, 0: 15})"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_test_pred = defaultdict(int)\n",
    "for y in y_test_pred:\n",
    "    label_test_pred[y] += 1\n",
    "    \n",
    "label_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 8406, -1: 123, 0: 15})"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_test_pred = defaultdict(int)\n",
    "for y in y_test_pred:\n",
    "    label_test_pred[y] += 1\n",
    "\n",
    "label_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set:  0.4327013108614232\n",
      "Accuracy on test set:  0.4327013108614232\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on train set: \", accuracy(y_train_pred, y_train))\n",
    "print(\"Accuracy on test set: \", accuracy(y_test_pred, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
