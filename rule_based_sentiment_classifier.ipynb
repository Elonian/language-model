{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries \n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Data\n",
    "\n",
    "using (data/sst-sentiment-text-threeclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(dirc: str) -> tuple[list[str], list[int]]:\n",
    "    input_X = []\n",
    "    output_y = []\n",
    "    with open(dirc, 'r') as file:\n",
    "        for line in file:\n",
    "            label, data = line.strip().split('|||')\n",
    "            input_X.append(data)\n",
    "            output_y.append(int(label))\n",
    "    return input_X, output_y\n",
    "\n",
    "X_train, Y_train = extract_data(\"/Users/varunmoparthi/Desktop/language-model/data/sst-sentiment-text-threeclass/train.txt\")\n",
    "X_test, Y_test = extract_data(\"/Users/varunmoparthi/Desktop/language-model/data/sst-sentiment-text-threeclass/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8544, 2210)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''''\n",
    "    Extract the feature for every review as a dictonary \n",
    "    containing good_word_count, bad_word_count, bias.\n",
    "'''''\n",
    "\n",
    "def feature_extract(text: str) -> defaultdict[str, int]:\n",
    "    feature = defaultdict()\n",
    "    \n",
    "    # These words are manually added (like a manual rule for considering which one is good and bad)\n",
    "    good_words = [\"good\"]\n",
    "    bad_words = [\"bad\"]\n",
    "\n",
    "    text_words  = text.split(' ')\n",
    "\n",
    "    for word in text_words:\n",
    "        if word in good_words:\n",
    "            feature[\"good_word_count\"] = feature.get(\"good_word_count\", 0) + 1\n",
    "        if word in bad_words:\n",
    "            feature[\"bad_word_count\"] = feature.get(\"bad_word_count\", 0) + 1\n",
    "    feature[\"bias\"] = 1\n",
    "\n",
    "    return feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Weights (Fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## These weights can be changed manually to tune the performance\n",
    "\n",
    "fetaure_weights = { \"good_word_count\": 1.0,\n",
    "                   \"bad_word_count\": -1.0, \n",
    "                   \"bias\": 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''''\n",
    "    score is calculated by dot product of weights and fetaure values.\n",
    "    Score_X = Weights . X_feature\n",
    "'''''\n",
    "\n",
    "def score_calculation(weights: defaultdict, feature: defaultdict) -> float:\n",
    "    score = 0.0\n",
    "    for k, v in weights.items():\n",
    "        score = score + weights.get(k, 0.0) * feature.get(k, 0.0)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Decision Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''''\n",
    "    Depending on the score of the review it will be classifed as 1, -1, 0 .\n",
    "'''''\n",
    "def decision_function(x: str) -> int:\n",
    "    feature = feature_extract(x)\n",
    "    score = score_calculation(fetaure_weights, feature)\n",
    "\n",
    "    if score > 0:\n",
    "        return 1\n",
    "    elif score < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Accuracy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(Y_pred: list[int], Y: list[int]) -> float:\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for pred, orig in zip(Y_pred, Y):\n",
    "        total += 1\n",
    "        if pred == orig:\n",
    "            correct += 1\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(data: list[int], type: str) -> None:\n",
    "    label = defaultdict()\n",
    "    for i in data:\n",
    "        label[i] = label.get(i, 0) + 1\n",
    "    print(f\"Dataset distribution {type}: \", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset distribution trainset:  defaultdict(None, {1: 3610, 0: 1624, -1: 3310})\n",
      "Dataset distribution testset:  defaultdict(None, {0: 389, 1: 909, -1: 912})\n"
     ]
    }
   ],
   "source": [
    "evaluate_dataset(Y_train, \"trainset\")\n",
    "evaluate_dataset(Y_test, \"testset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset accuracy:  0.4327013108614232\n",
      "Testset accuracy:  0.4239819004524887\n"
     ]
    }
   ],
   "source": [
    "Y_train_pred = []\n",
    "for text in X_train:\n",
    "    Y_train_pred.append(decision_function(text))\n",
    "\n",
    "Y_test_pred = []\n",
    "for text in X_test:\n",
    "    Y_test_pred.append(decision_function(text))\n",
    "\n",
    "print(\"Trainset accuracy: \", calculate_accuracy(Y_train_pred, Y_train))\n",
    "print(\"Testset accuracy: \", calculate_accuracy(Y_test_pred, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above accuracy is better than random (meaning if we slect yes every time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4113122171945701"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "909/ (909 + 912 + 389)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve the accuracy : \n",
    "1. Add more good and bad words\n",
    "2. Change the weights in feature_weights after looking at the mistakes (doing error analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''''\n",
    "    Find the random 10 text where the perdcition is wrong and checking the text and its label \n",
    "'''''\n",
    "def error_analysis(X: list[str], Y: list[int]) -> None:\n",
    "    Y_pred = []\n",
    "    error = []\n",
    "    for i, (text, y) in enumerate(zip(X, Y)):\n",
    "        Y_pred.append(decision_function(text))\n",
    "        if Y[i] != Y_pred[-1]:\n",
    "            error.append(i)\n",
    "    \n",
    "    for _ in range(10):\n",
    "        id = random.choice(error)\n",
    "        print(\"Text: \", X[id])\n",
    "        print(\"Predicted: \", Y_pred[id])\n",
    "        print(\"Original: \", Y[id], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:   An instant candidate for worst movie of the year .\n",
      "Predicted:  1\n",
      "Original:  -1 \n",
      "\n",
      "Text:   Some decent actors inflict big damage upon their reputations .\n",
      "Predicted:  1\n",
      "Original:  -1 \n",
      "\n",
      "Text:   All I can say is fuhgeddaboutit .\n",
      "Predicted:  1\n",
      "Original:  -1 \n",
      "\n",
      "Text:   Not a cheap slasher flick , as the subject matter would suggest , but is a little like a nature film , showing a patient predator and his foolish prey .\n",
      "Predicted:  1\n",
      "Original:  0 \n",
      "\n",
      "Text:   Adam Sandler is to Gary Cooper what a gnat is to a racehorse .\n",
      "Predicted:  1\n",
      "Original:  -1 \n",
      "\n",
      "Text:   It does n't help that the director and cinematographer Stephen Kazmierski shoot on grungy video , giving the whole thing a dirty , tasteless feel .\n",
      "Predicted:  1\n",
      "Original:  -1 \n",
      "\n",
      "Text:   Neither funny nor suspenseful nor particularly well-drawn .\n",
      "Predicted:  1\n",
      "Original:  -1 \n",
      "\n",
      "Text:   But like Bruce Springsteen 's gone-to-pot Asbury Park , New Jersey , this sad-sack waste of a movie is a City of ruins .\n",
      "Predicted:  1\n",
      "Original:  -1 \n",
      "\n",
      "Text:   Accuracy and realism are terrific , but if your film becomes boring , and your dialogue is n't smart , then you need to use more poetic license .\n",
      "Predicted:  1\n",
      "Original:  -1 \n",
      "\n",
      "Text:   A few artsy flourishes aside , Narc is as gritty as a movie gets these days .\n",
      "Predicted:  1\n",
      "Original:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "error_analysis(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
